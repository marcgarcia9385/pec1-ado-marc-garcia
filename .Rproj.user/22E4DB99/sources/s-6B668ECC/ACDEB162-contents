# K-NN para Clasificación:

# -------------------------------------------------------------------------------------------------------------------------------------------------

# Estableciendo el directorio de trabajo

setwd("C:/Users/USER/Documents/Màster Bioestadística i Bioinfo/Machine Learning/PEC1/data")

#-------------------------------------------------------------------------------------------------------------------------------------------------

# Carga de paquetes

library(class)
library(caret)
library(caTools)
library(gmodels)

# -------------------------------------------------------------------------------------------------------------------------------------------------

# Importar el dataset

dataset <- read.csv("RX_Torax_4097.csv", stringsAsFactors = F)

# --------------------------------------------------------------------------------------------------------------------------------------------------

# Resumen estadístico (análisis exploratorio)

str(cancer)
summary(cancer)
summary(cancer[c("concavity_mean", "concave.points_mean")])
summary(cancer$diagnosis)

# ------------------------------------------------------------------------------------------------------------------------------------------------

# Preprocesado (eliminación de variables, factorización, escalado)

cancer <- cancer[ , -1]
cancer$diagnosis <- factor(cancer$diagnosis, 
                           levels = c("B", "M"), 
                           labels = c("Benign", "Malignant"))

# Opción A -> Normalización 

# * el más utilizado para el algoritmo K-NN

normalize <- function(x){
  return((x - min(x))/(max(x) - min(x)))
}

cancer.n <- as.data.frame(lapply(X = cancer[2:31], FUN = normalize))
cancer.n <- cbind(diagnosis = cancer[ , 1], cancer.n)
summary(cancer.n)                               # Min = 0, Max = 1

# Opción B -> Estandarización z

cancer.z <- cancer
cancer.z[ , -1] <- scale(cancer.z[ , -1])
summary(cancer.z)                               # Medias = 0

# ---------------------------------------------------------------------------------------------------------------------------------------------------

# Dividir los datos en conjunto de entrenamiento (training_set) y conjunto de validación (testing_set)

# Opción A

set.seed(123)

t.ids1 <- sample.split(Y = cancer$diagnosis, SplitRatio = 400/569)
t.set1 <- cancer.n[t.ids1 == TRUE, ]
v.set1 <- cancer.n[t.ids1 == FALSE, ]
  
# Opción B

set.seed(123)

t.ids2 <- createDataPartition(y = cancer$diagnosis, p = 400/569, list = F)
t.set2 <- cancer.z[t.ids2, ]
v.set2 <- cancer.z[-t.ids2, ]

# ------------------------------------------------------------------------------------------------------------------------------------------------

# Hacer las Predicciones con el conjunto de validación (en un solo paso)

y.pred1 <- knn(train = t.set1[ , -1], 
              test = v.set1[ , -1], 
              cl = t.set1[ , 1], 
              k = 21)

y.pred1

y.pred2 <- knn(train = t.set2[ , -1], 
               test = v.set2[ , -1], 
               cl = t.set2[ , 1],
               k = 21)

y.pred2

# --------------------------------------------------------------------------------------------------------------------------------

# Crear la matriz de Confusión

# Opción A

cm1.a <- table(v.set1$diagnosis, y.pred1)
cm1.a
per_succ1.a <- ((cm1[1, 1] + cm1[2, 2])/sum(cm1)) * 100
per_succ1.a

cm2.a <- table(v.set2$diagnosis, y.pred2)
cm2.a
per_succ2.a <- ((cm2.a[1, 1] + cm2.a[2, 2])/sum(cm2-a)) * 100
per_succ2.a

# Opción B

cm1.b <- confusionMatrix(y_pred1, v.set1$diagnosis)
cm1.b
cm2.b <- confusionMatrix(y_pred2, v.set2$diagnosis)


# Opción C

cm1.c <- CrossTable(x = v.set1$diagnosis, 
                    y = y_pred1, 
                    prop.chisq = F, 
                    dnn = c("Actual", "Predicted"))

cm2.c <- CrossTable(x = v.set2$diagnosis, 
                    y = y_pred2, 
                    prop.chisq = F, 
                    dnn = c("Actual", "Predicted"))

# ---------------------------------------------------------------------------------------------------------------------------------

# Mejorando el modelo

# * Cambiar el método de estandarizar los datos (ya lo hemos hecho)
# * Variar el valor de k

